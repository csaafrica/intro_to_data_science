{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3e5a0c",
   "metadata": {},
   "source": [
    "# Day 2: Data Manipulation with Pandas and GitHub Collaboration\n",
    "\n",
    "**Prepared By:** Dr. Kenechi Omeke  \n",
    "**Date:** November 2024  \n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Overview\n",
    "Today, you’ll learn how to wrangle real-world data using Pandas and collaborate with others using GitHub. These are foundational skills for any data scientist, enabling you to clean, analyze, and share your work with confidence.\n",
    "\n",
    "## Aim\n",
    "Equip students with skills to manipulate and analyze data using Pandas, and collaborate using GitHub.\n",
    "\n",
    "## Intended Learning Outcomes\n",
    "By the end of this chapter, you will be able to:\n",
    "- Load data into Pandas DataFrames and perform basic and intermediate data manipulation.\n",
    "- Clean and preprocess real-world datasets, understanding the theory behind each operation.\n",
    "- Use GitHub to collaborate and share code, understanding the history and impact of version control.\n",
    "- Clone a repository, create branches, and push/pull changes.\n",
    "- Use Git with Jupyter Notebooks effectively and avoid common pitfalls.\n",
    "\n",
    "## Why This Matters\n",
    "Most of your time as a data scientist will be spent cleaning and transforming data. Pandas is the industry-standard tool for this. GitHub is the backbone of collaborative coding and reproducible science. Mastering these tools is essential for both solo and team projects.\n",
    "\n",
    "## Topics Covered\n",
    "- Introduction to Pandas: History, Philosophy, and Ecosystem\n",
    "- DataFrames and Series (with code examples and advanced selection)\n",
    "- Data Cleaning and Preprocessing (hands-on, with theory)\n",
    "- Case Study: Data Cleaning in the Real World\n",
    "- GitHub Basics: Repositories, Cloning, Pushing, and Pulling\n",
    "- Using Git with Jupyter Notebooks\n",
    "- Mini-Project: Data Cleaning and Collaboration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a Series (1D labeled array)\n",
    "data = pd.Series([10, 20, 30, 40], index=['A', 'B', 'C', 'D'])\n",
    "print(\"Series example:\")\n",
    "print(data)\n",
    "\n",
    "# Create a DataFrame (2D labeled table)\n",
    "data_dict = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "             'Age': [24, 27, 22],\n",
    "             'City': ['Nairobi', 'Mombasa', 'Kisumu']}\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(\"\\nDataFrame example:\")\n",
    "print(df)\n",
    "\n",
    "# Advanced: Selecting rows and columns\n",
    "print(\"\\nSelect 'Age' column:\")\n",
    "print(df['Age'])\n",
    "print(\"\\nSelect first two rows:\")\n",
    "print(df.iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e92bd",
   "metadata": {},
   "source": [
    "## Pandas: History and Philosophy\n",
    "\n",
    "Pandas was created by Wes McKinney in 2008 to fill a gap in Python’s data analysis capabilities. Its name comes from \"Panel Data\" (econometrics) and \"Python Data Analysis\". Today, it is the foundation of the Python data science ecosystem.\n",
    "\n",
    "**Philosophy:**\n",
    "- Make working with tabular data fast, flexible, and expressive.\n",
    "- Provide powerful tools for cleaning, transforming, and analyzing data.\n",
    "\n",
    "**Key Data Structures:**\n",
    "- **Series:** 1D labeled array (like a column)\n",
    "- **DataFrame:** 2D labeled table (like a spreadsheet)\n",
    "\n",
    "**Why not just use lists or dictionaries?**\n",
    "- Pandas is optimized for performance and memory.\n",
    "- Built-in methods for filtering, grouping, joining, and reshaping data.\n",
    "- Integrates with NumPy, Matplotlib, and other libraries.\n",
    "\n",
    "**Diagram:**\n",
    "```\n",
    "[Series]   [Series]   [Series]\n",
    "    |         |         |\n",
    "    +----> [DataFrame] <----+\n",
    "```\n",
    "\n",
    "**Key Takeaway:**\n",
    "Pandas lets you work with data at scale, with the power of SQL and the flexibility of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0cee30",
   "metadata": {},
   "source": [
    "# 2. Loading and Inspecting Data\n",
    "\n",
    "Real-world data often comes in CSV, Excel, or database formats. Pandas makes it easy to load and inspect data.\n",
    "\n",
    "**Best Practice:**\n",
    "- Always inspect your data before analysis: check for missing values, data types, and outliers.\n",
    "- Use `df.head()`, `df.info()`, and `df.describe()` as your first steps.\n",
    "\n",
    "**Diagram:**\n",
    "```\n",
    "[Raw Data File] → [pd.read_csv()] → [DataFrame] → [df.head(), df.info(), df.describe()]\n",
    "```\n",
    "\n",
    "**Advanced Tip:**\n",
    "- Use `df.sample(5)` to see a random sample of your data.\n",
    "- Use `df.dtypes` to check column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset from seaborn\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('titanic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f00fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Advanced: Check for unique values in categorical columns\n",
    "print(\"\\nUnique embarkation ports:\", df['embarked'].unique())\n",
    "\n",
    "# **What to look for:**\n",
    "# - info() shows data types and missing values\n",
    "# - describe() gives summary stats for numeric columns\n",
    "# - isna().sum() counts missing values per column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55862b5b",
   "metadata": {},
   "source": [
    "## Exercise: Data Inspection\n",
    "\n",
    "- How many missing values are in the `age` column?\n",
    "- What is the average fare paid by passengers?\n",
    "- Are there any columns with lots of missing data?\n",
    "- What are the unique values in the `embarked` column?\n",
    "\n",
    "**Reflection:**\n",
    "- Why is it important to check for missing values before analysis?\n",
    "- How might missing data affect your results?\n",
    "- What would you do if a column has >50% missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ceeff",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Real-world data is messy! Cleaning is the process of fixing or removing incorrect, corrupted, or incomplete data.\n",
    "\n",
    "**Common Tasks:**\n",
    "- Filling or dropping missing values\n",
    "- Filtering rows or columns\n",
    "- Changing data types\n",
    "- Removing duplicates\n",
    "- Standardizing text (e.g., lowercasing, trimming whitespace)\n",
    "\n",
    "**Theory:**\n",
    "- Imputation (filling missing values) can introduce bias if not done carefully.\n",
    "- Dropping rows reduces data size and may remove important information.\n",
    "- Always document your cleaning steps for reproducibility.\n",
    "\n",
    "**Best Practice:**\n",
    "- Never delete data unless you have a good reason.\n",
    "- Use `df.copy()` before making major changes.\n",
    "\n",
    "**Diagram:**\n",
    "```\n",
    "[Raw Data] → [Cleaning Steps] → [Clean DataFrame]\n",
    "```\n",
    "\n",
    "**Case Study:**\n",
    "Imagine you’re working for a hospital analyzing patient data. Missing ages could be imputed with the median, but if most missing values are from a specific ward, you might be introducing bias. Always investigate patterns in missingness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34adf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing ages with the median\n",
    "median_age = df['age'].median()\n",
    "df['age'].fillna(median_age, inplace=True)\n",
    "\n",
    "# Drop rows where 'embarked' is missing\n",
    "df = df.dropna(subset=['embarked'])\n",
    "\n",
    "# Remove duplicate rows (if any)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Check again\n",
    "print(df.isna().sum())\n",
    "\n",
    "# **What to look for:**\n",
    "# - Fewer missing values after cleaning\n",
    "# - Document your choices (why median? why drop rows?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: Find all female passengers from 'Cherbourg' who survived\n",
    "filtered = df[(df['sex'] == 'female') & (df['embarked'] == 'C') & (df['survived'] == 1)]\n",
    "print(filtered[['name', 'age', 'fare']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c875dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and summarizing: Average age by passenger class\n",
    "avg_age_by_class = df.groupby('pclass')['age'].mean()\n",
    "print(avg_age_by_class)\n",
    "\n",
    "# Advanced: Pivot table for survival rate by class and gender\n",
    "pivot = df.pivot_table(index='pclass', columns='sex', values='survived', aggfunc='mean')\n",
    "print(\"\\nSurvival rate by class and gender:\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c36d332",
   "metadata": {},
   "source": [
    "## Exercise: Data Cleaning\n",
    "\n",
    "1. Fill missing values in the `fare` column with the mean fare.\n",
    "2. How many passengers are in each embarkation port? (Hint: use `value_counts()`)\n",
    "3. Remove duplicate rows (if any) and check the shape of the DataFrame.\n",
    "4. Standardize the `sex` column to lowercase.\n",
    "5. Create a new column for \"age group\" (e.g., child, adult, senior).\n",
    "\n",
    "**Reflection:**\n",
    "- What are the risks of dropping rows with missing data?\n",
    "- When is it better to impute (fill) missing values?\n",
    "- How does cleaning affect downstream analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7538b",
   "metadata": {},
   "source": [
    "# 4. GitHub Collaboration Basics\n",
    "\n",
    "GitHub lets you share code and collaborate with others. It’s the standard platform for open-source and team projects.\n",
    "\n",
    "**A Brief History:**\n",
    "- Git was created by Linus Torvalds in 2005 for Linux kernel development.\n",
    "- GitHub launched in 2008 and revolutionized open-source collaboration.\n",
    "\n",
    "**Why GitHub?**\n",
    "- Enables distributed, asynchronous teamwork\n",
    "- Tracks every change and makes collaboration transparent\n",
    "- Integrates with CI/CD, issue tracking, and more\n",
    "\n",
    "**Common GitHub Tasks:**\n",
    "- **Clone a repository:**\n",
    "  ```bash\n",
    "  git clone https://github.com/your-username/your-repo.git\n",
    "  ```\n",
    "- **Create a new branch:**\n",
    "  ```bash\n",
    "  git checkout -b data-cleaning\n",
    "  ```\n",
    "- **Stage, commit, and push changes:**\n",
    "  ```bash\n",
    "  git add your_notebook.ipynb\n",
    "  git commit -m \"Cleaned Titanic dataset\"\n",
    "  git push origin data-cleaning\n",
    "  ```\n",
    "- **Pull latest changes:**\n",
    "  ```bash\n",
    "  git pull origin main\n",
    "  ```\n",
    "\n",
    "**Best Practice:**\n",
    "- Use branches for new features or experiments\n",
    "- Write clear commit messages\n",
    "- Review code before merging\n",
    "- Use pull requests for code review\n",
    "\n",
    "**Diagram:**\n",
    "```\n",
    "[Local Repo] ←→ [GitHub Repo]\n",
    "   |                |\n",
    " [Branch]        [Pull Request]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c273ac3",
   "metadata": {},
   "source": [
    "# 5. Using Git with Jupyter Notebooks\n",
    "\n",
    "- Always clear output before committing (to avoid large diffs):\n",
    "  ```bash\n",
    "  jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace my_notebook.ipynb\n",
    "  ```\n",
    "- Add `.ipynb_checkpoints/` to `.gitignore`.\n",
    "- Use meaningful commit messages.\n",
    "- Use `nbdime` for notebook diffs and merges: https://nbdime.readthedocs.io/\n",
    "\n",
    "**Tip:** Collaborate by using branches and pull requests.\n",
    "\n",
    "**Advanced Tip:**\n",
    "- Use GitHub Actions to automate testing or notebook execution.\n",
    "\n",
    "**Key Takeaway:**\n",
    "Version control is just as important for notebooks as for code. Good practices prevent merge conflicts and lost work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5db228",
   "metadata": {},
   "source": [
    "# 6. Mini-Project: Titanic Data Cleaning & Collaboration\n",
    "\n",
    "In this mini-project, you’ll apply your cleaning and collaboration skills:\n",
    "\n",
    "1. Load the Titanic dataset (or another CSV if you prefer).\n",
    "2. Clean the data: handle missing values, filter, and summarize.\n",
    "3. Save your cleaned DataFrame to a new CSV file.\n",
    "4. Push your notebook and CSV to a GitHub repository.\n",
    "5. Share your repo link with a classmate and review each other's work.\n",
    "\n",
    "**Case Study Narrative:**\n",
    "Imagine you’re part of a team at a data consultancy. Your client needs a clean dataset for a survival analysis. You must document every step, justify your choices, and collaborate with a teammate who will review your work and suggest improvements.\n",
    "\n",
    "**Reflection:**\n",
    "- What was the most challenging part of cleaning the data?\n",
    "- How did collaboration change your workflow?\n",
    "- What would you do differently if you had more time or data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cc0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned DataFrame to CSV\n",
    "# (Uncomment the next line to save if running locally)\n",
    "# df.to_csv('titanic_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649eb669",
   "metadata": {},
   "source": [
    "## Reflection & Next Steps\n",
    "\n",
    "- What was the most challenging part of cleaning the data?\n",
    "- Try loading a different dataset and repeat the process.\n",
    "- Explore more Pandas functions: `pivot_table`, `merge`, `apply`, `melt`, `cut`.\n",
    "- Try using GitHub Issues to track your project tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "- Pandas is your main tool for data wrangling and analysis.\n",
    "- Clean, well-documented data is the foundation of good science.\n",
    "- GitHub enables collaboration and reproducibility.\n",
    "- Practice, curiosity, and clear documentation are your best friends.\n",
    "\n",
    "## Further Reading & Resources\n",
    "- Wes McKinney, *Python for Data Analysis*, O'Reilly Media\n",
    "- [Pandas Documentation](https://pandas.pydata.org/)\n",
    "- [GitHub Docs](https://docs.github.com/en/get-started)\n",
    "- [Real Python Pandas Tutorials](https://realpython.com/tutorials/pandas/)\n",
    "- [nbdime: Notebook Diffing](https://nbdime.readthedocs.io/)\n",
    "- [Tidy Data by Hadley Wickham](https://vita.had.co.nz/papers/tidy-data.pdf)\n",
    "- [Data Science at the Command Line](https://datascienceatthecommandline.com/)\n",
    "\n",
    "---\n",
    "\n",
    "**Great job! You’ve completed your second chapter in data science.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdf0da",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
